"""
Service LLM pour g√©n√©ration de synth√®ses de trading
Limite les tokens pour des r√©ponses concises et efficaces
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional

import requests
from loguru import logger

from src.constants import CONSTANTS


class LLMService:
    """Service LLM avec limitation de tokens pour synth√®ses efficaces"""

    def __init__(self):
        self.ollama_url = CONSTANTS.OLLAMA_URL
        self.model = CONSTANTS.OLLAMA_MODEL
        self.max_tokens = CONSTANTS.OLLAMA_MAX_TOKENS
        self.max_context = 2000  # Limite du contexte d'entr√©e

        logger.info("üß† Service LLM initialis√©")

    def generate_trading_synthesis(
        self, ticker: str, recommendation: str, fusion_score: float, price: float, sentiment_score: float = None
    ) -> Dict:
        """G√©n√®re une synth√®se concise de la d√©cision de trading"""
        try:
            # Construire le prompt concis
            prompt = self._build_concise_prompt(ticker, recommendation, fusion_score, price, sentiment_score)

            # Appel √† Ollama
            response = self._call_ollama(prompt)

            if response:
                return {
                    "success": True,
                    "synthesis": response,
                    "model": self.model,
                    "timestamp": datetime.now().isoformat(),
                    "tokens_used": len(response.split()),  # Estimation
                }
            else:
                return {
                    "success": False,
                    "synthesis": "Service LLM indisponible",
                    "model": self.model,
                    "timestamp": datetime.now().isoformat(),
                    "tokens_used": 0,
                }

        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration synth√®se: {e}")
            return {
                "success": False,
                "synthesis": f"Erreur: {str(e)}",
                "model": self.model,
                "timestamp": datetime.now().isoformat(),
                "tokens_used": 0,
            }

    def _build_concise_prompt(
        self, ticker: str, recommendation: str, fusion_score: float, price: float, sentiment_score: float = None
    ) -> str:
        """Construit un prompt concis pour la synth√®se"""

        # Informations de base
        base_info = f"Ticker: {ticker}, Prix: ${price:.2f}, Recommandation: {recommendation}, Score: {fusion_score:.3f}"

        # Ajouter sentiment si disponible
        if sentiment_score is not None:
            base_info += f", Sentiment: {sentiment_score:.3f}"

        # Prompt concis avec limitation stricte
        prompt = f"""
Analyse trading concise (max 100 mots):

{base_info}

Synth√®se: Explique bri√®vement pourquoi {recommendation} pour {ticker}.
Focus: Score de fusion, tendance prix, sentiment.
Format: 2-3 phrases maximum.
"""

        return prompt.strip()

    def _call_ollama(self, prompt: str) -> Optional[str]:
        """Appelle l'API Ollama avec limitation de tokens"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": self.max_tokens,  # Limite stricte
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "stop": ["\n\n", "---", "##"],  # Arr√™ts pour concision
                },
            }

            response = requests.post(
                self.ollama_url, json=payload, timeout=10  # Timeout r√©duit pour √©viter les blocages
            )

            if response.status_code == 200:
                result = response.json()
                synthesis = result.get("response", "").strip()

                # V√©rifier la longueur
                if len(synthesis.split()) > self.max_tokens:
                    synthesis = " ".join(synthesis.split()[: self.max_tokens]) + "..."

                logger.info(f"‚úÖ Synth√®se g√©n√©r√©e: {len(synthesis.split())} mots")
                return synthesis
            else:
                logger.warning(f"‚ö†Ô∏è Erreur API Ollama: {response.status_code}")
                return None

        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Erreur connexion Ollama: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erreur appel Ollama: {e}")
            return None

    def check_service_status(self) -> Dict:
        """V√©rifie le statut du service Ollama"""
        try:
            response = requests.get(CONSTANTS.OLLAMA_TAGS_URL, timeout=5)

            if response.status_code == 200:
                models = response.json().get("models", [])
                phi3_available = any("phi3" in model.get("name", "") for model in models)

                return {
                    "online": True,
                    "model_available": phi3_available,
                    "models": [model.get("name", "") for model in models],
                    "model": "phi3:mini" if phi3_available else "N/A",
                    "status": "‚úÖ Service actif" if phi3_available else "‚ö†Ô∏è Mod√®le phi3 non trouv√©",
                }
            else:
                return {
                    "online": False,
                    "model_available": False,
                    "models": [],
                    "model": "N/A",
                    "status": "‚ùå Service indisponible",
                }

        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification statut: {e}")
            return {
                "online": False,
                "model_available": False,
                "models": [],
                "model": "N/A",
                "status": f"‚ùå Erreur: {str(e)}",
            }

    def save_synthesis(self, ticker: str, synthesis_data: Dict):
        """Sauvegarde une synth√®se dans les logs"""
        try:
            from pathlib import Path

            data_path = CONSTANTS.get_data_path()
            synthesis_path = data_path / "trading" / "llm_synthesis"
            synthesis_path.mkdir(parents=True, exist_ok=True)

            # Fichier par ticker et date
            filename = f"{ticker}_synthesis_{datetime.now().strftime('%Y%m%d')}.json"
            file_path = synthesis_path / filename

            # Charger les synth√®ses existantes
            if file_path.exists():
                with open(file_path, "r") as f:
                    syntheses = json.load(f)
            else:
                syntheses = []

            # Ajouter la nouvelle synth√®se
            syntheses.append(synthesis_data)

            # Sauvegarder
            with open(file_path, "w") as f:
                json.dump(syntheses, f, indent=2)

            logger.info(f"‚úÖ Synth√®se sauvegard√©e: {file_path}")

        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde synth√®se: {e}")

    def generate_automatic_synthesis(self, ticker: str, decision_data: Dict) -> Dict:
        """G√©n√®re automatiquement une synth√®se bas√©e sur une d√©cision de trading"""
        try:
            # Extraire les donn√©es de la d√©cision
            recommendation = decision_data.get("recommendation", decision_data.get("decision", "HOLD"))
            confidence = decision_data.get("confidence", 0.5)
            fused_signal = decision_data.get("fused_signal", decision_data.get("score", 0.0))
            signals = decision_data.get("signals", {})

            # Construire le prompt pour synth√®se automatique
            prompt = f"""
Analyse trading automatique (max 100 mots):

Ticker: {ticker}
D√©cision: {recommendation}
Confiance: {confidence:.1%}
Signal fusionn√©: {fused_signal:.3f}
Signaux: Prix={signals.get('price', 0):.3f}, Sentiment={signals.get('sentiment', 0):.3f}

Synth√®se: Explique pourquoi {recommendation} pour {ticker}.
Focus: Confiance √©lev√©e, signaux coh√©rents.
Format: 2-3 phrases maximum.
"""

            # Appel √† Ollama
            response = self._call_ollama(prompt)

            if response:
                synthesis_data = {
                    "success": True,
                    "synthesis": response,
                    "model": self.model,
                    "timestamp": datetime.now().isoformat(),
                    "tokens_used": len(response.split()),
                    "auto_generated": True,
                    "decision_timestamp": decision_data.get("timestamp", ""),
                    "confidence": confidence,
                    "fused_signal": fused_signal,
                }

                # Sauvegarder automatiquement
                self.save_synthesis(ticker, synthesis_data)

                return synthesis_data
            else:
                return {
                    "success": False,
                    "synthesis": "Synth√®se automatique indisponible",
                    "model": self.model,
                    "timestamp": datetime.now().isoformat(),
                    "tokens_used": 0,
                    "auto_generated": True,
                }

        except Exception as e:
            logger.error(f"‚ùå Erreur synth√®se automatique: {e}")
            return {
                "success": False,
                "synthesis": f"Erreur: {str(e)}",
                "model": self.model,
                "timestamp": datetime.now().isoformat(),
                "tokens_used": 0,
                "auto_generated": True,
            }
