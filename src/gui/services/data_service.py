"""
Service de donn√©es pour Streamlit
Chargement et filtrage des donn√©es historiques
"""

from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np
import pandas as pd
from loguru import logger


class DataService:
    """Service de donn√©es optimis√© pour Streamlit"""

    def __init__(self):
        from src.constants import CONSTANTS
        self.historical_path = CONSTANTS.YFINANCE_DIR
        self.realtime_path = CONSTANTS.PRICES_DIR
        self.features_dir = CONSTANTS.FEATURES_DIR
        self.cache = {}
        logger.info("üìä Service de donn√©es initialis√©")

    def load_data(self, ticker: str, use_historical: bool = False, use_features: bool = False) -> pd.DataFrame:
        """Charge les donn√©es pour un ticker (features en priorit√© pour LSTM)"""
        try:
            cache_key = f"{ticker}_{'features' if use_features else 'historical' if use_historical else 'combined'}"
            if cache_key in self.cache:
                return self.cache[cache_key]

            if use_features:
                # Pour le mod√®le LSTM, charger les features techniques
                features_file = self.features_dir / f"{ticker.lower()}_features.parquet"
                if features_file.exists():
                    df = self._load_features_data(features_file, ticker)
                    if not df.empty:
                        self.cache[cache_key] = df
                        logger.info(f"‚úÖ Features charg√©es pour {ticker}: {len(df)} lignes")
                        return df

            if use_historical:
                # Pour les analyses long terme, combiner historique + temps r√©el
                df_combined = self._load_combined_data(ticker)
                if not df_combined.empty:
                    self.cache[cache_key] = df_combined
                    logger.info(f"‚úÖ Donn√©es combin√©es charg√©es pour {ticker}: {len(df_combined)} lignes")
                    return df_combined
            else:
                # Essayer d'abord les donn√©es temps r√©el
                realtime_file = self.realtime_path / f"{ticker.lower()}_15min.parquet"
                if realtime_file.exists():
                    df = self._load_realtime_data(realtime_file, ticker)
                    if not df.empty:
                        self.cache[cache_key] = df
                        logger.info(f"‚úÖ Donn√©es temps r√©el charg√©es pour {ticker}: {len(df)} lignes")
                        return df

                # Fallback vers les donn√©es combin√©es
                df_combined = self._load_combined_data(ticker)
                if not df_combined.empty:
                    self.cache[cache_key] = df_combined
                    logger.info(f"‚úÖ Donn√©es combin√©es charg√©es pour {ticker}: {len(df_combined)} lignes")
                    return df_combined

            logger.error(f"‚ùå Aucune donn√©e trouv√©e pour {ticker}")
            return pd.DataFrame()

        except Exception as e:
            logger.error(f"‚ùå Erreur chargement {ticker}: {e}")
            return pd.DataFrame()

    def _load_combined_data(self, ticker: str) -> pd.DataFrame:
        """Combine les donn√©es historiques et temps r√©el pour avoir les donn√©es les plus r√©centes"""
        try:
            # Charger les donn√©es historiques
            historical_file = self.historical_path / f"{ticker}_1999_2025.parquet"
            df_hist = pd.DataFrame()
            if historical_file.exists():
                df_hist = self._load_historical_data(historical_file, ticker)

            # Charger les donn√©es temps r√©el
            realtime_file = self.realtime_path / f"{ticker.lower()}_15min.parquet"
            df_realtime = pd.DataFrame()
            if realtime_file.exists():
                df_realtime = self._load_realtime_data(realtime_file, ticker)

            if df_hist.empty and df_realtime.empty:
                return pd.DataFrame()

            if df_hist.empty:
                return df_realtime

            if df_realtime.empty:
                return df_hist

            # Combiner les donn√©es en √©vitant les doublons
            # Utiliser les donn√©es temps r√©el pour les dates r√©centes
            last_hist_date = df_hist.index.max()
            first_realtime_date = df_realtime.index.min()

            # Garder les donn√©es historiques jusqu'√† la veille des donn√©es temps r√©el
            df_hist_filtered = df_hist[df_hist.index < first_realtime_date]

            # Combiner
            df_combined = pd.concat([df_hist_filtered, df_realtime])
            df_combined = df_combined.sort_index()

            # Supprimer les doublons potentiels
            df_combined = df_combined[~df_combined.index.duplicated(keep="last")]

            logger.info(
                f"‚úÖ Donn√©es combin√©es: {len(df_hist_filtered)} historiques + {len(df_realtime)} temps r√©el = {len(df_combined)} total"
            )
            return df_combined

        except Exception as e:
            logger.error(f"‚ùå Erreur combinaison donn√©es {ticker}: {e}")
            return pd.DataFrame()

    def _load_realtime_data(self, file_path: Path, ticker: str) -> pd.DataFrame:
        """Charge les donn√©es temps r√©el"""
        df = pd.read_parquet(file_path)

        # Convertir ts_utc en DATE et l'utiliser comme index
        df["DATE"] = pd.to_datetime(df["ts_utc"], utc=True)
        df = df.set_index("DATE")

        # Normalisation des colonnes (majuscules)
        df.columns = df.columns.str.upper()

        # Garder seulement les colonnes n√©cessaires
        keep_cols = ["OPEN", "HIGH", "LOW", "CLOSE", "VOLUME"]
        df = df[[col for col in keep_cols if col in df.columns]]

        # Tri par date
        df = df.sort_index()

        return self._validate_data(df, ticker)

    def _load_features_data(self, file_path: Path, ticker: str) -> pd.DataFrame:
        """Charge les features techniques pour le mod√®le LSTM"""
        df = pd.read_parquet(file_path)

        # Filtrer par ticker
        if "TICKER" in df.columns:
            df = df[df["TICKER"] == ticker].copy()
            df = df.drop("TICKER", axis=1)

        # Utiliser DATE comme index
        if "DATE" in df.columns:
            df["DATE"] = pd.to_datetime(df["DATE"], utc=True)
            df = df.set_index("DATE")

        # Normalisation des colonnes (majuscules)
        df.columns = df.columns.str.upper()

        # Garder toutes les features techniques n√©cessaires pour LSTM (en majuscules)
        feature_cols = [
            "RSI_14",
            "MACD",
            "BB_POSITION",
            "STOCH_K",
            "WILLIAMS_R",
            "EMA_RATIO",
            "ATR_NORMALIZED",
            "VOLUME_RATIO",
            "PRICE_POSITION",
            "ROC_10",
            "RETURNS",
            "RETURNS_MA_5",
            "RETURNS_MA_10",
            "RETURNS_MA_20",
            "RETURNS_MA_50",
            "MOMENTUM_5",
            "MOMENTUM_10",
            "MOMENTUM_20",
            "VOLUME_PRICE_TREND",
            "PRICE_VELOCITY",
            "PRICE_POSITION_20",
        ]

        # Garder les colonnes disponibles
        available_cols = [col for col in feature_cols if col in df.columns]
        if available_cols:
            df = df[available_cols]
        else:
            # Fallback vers les colonnes OHLCV si pas de features
            fallback_cols = ["OPEN", "HIGH", "LOW", "CLOSE", "VOLUME"]
            df = df[[col for col in fallback_cols if col in df.columns]]

        # Tri par date
        df = df.sort_index()

        # Nettoyer les NaN
        df = df.ffill().bfill()

        # Normaliser les colonnes en majuscules pour correspondre aux features attendues
        df.columns = df.columns.str.upper()

        logger.info(f"‚úÖ Features charg√©es pour {ticker}: {len(df)} lignes, {len(df.columns)} colonnes")
        return df

    def _load_historical_data(self, file_path: Path, ticker: str) -> pd.DataFrame:
        """Charge les donn√©es historiques"""
        df = pd.read_parquet(file_path)

        # Normalisation des colonnes (majuscules)
        df.columns = df.columns.str.upper()

        # Filtrer par ticker si n√©cessaire
        if "TICKER" in df.columns:
            df = df[df["TICKER"] == ticker].copy()
            df = df.drop("TICKER", axis=1)

        # Conversion des dates en UTC
        if "DATE" in df.columns:
            df["DATE"] = pd.to_datetime(df["DATE"], utc=True)
            df = df.set_index("DATE")

        # Tri par date
        df = df.sort_index()

        return self._validate_data(df, ticker)

    def _validate_data(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:
        """Valide et nettoie les donn√©es"""
        if df.empty:
            return df

        # V√©rifier les colonnes requises (CLOSE et VOLUME, DATE est l'index)
        required_cols = ["CLOSE", "VOLUME"]
        missing_cols = [col for col in required_cols if col not in df.columns]

        if missing_cols:
            logger.warning(f"‚ö†Ô∏è Colonnes manquantes pour {ticker}: {missing_cols}")
            return pd.DataFrame()

        # Nettoyer les valeurs NaN/Inf
        df = df.replace([np.inf, -np.inf], np.nan)

        # Supprimer les lignes avec des prix invalides
        df = df.dropna(subset=["CLOSE"])

        # V√©rifier que les prix sont positifs
        df = df[df["CLOSE"] > 0]

        if df.empty:
            logger.error(f"‚ùå Aucune donn√©e valide pour {ticker}")
            return pd.DataFrame()

        logger.info(f"‚úÖ Donn√©es valid√©es pour {ticker}: {len(df)} lignes valides")
        return df

    def filter_by_period(self, df: pd.DataFrame, period: str) -> pd.DataFrame:
        """Filtre les donn√©es par p√©riode"""
        if df.empty:
            return df

        periods = {
            "7 derniers jours": 7,
            "1 mois": 30,
            "3 mois": 90,
            "6 derniers mois": 180,
            "1 an": 365,
            "3 ans": 1095,
            "5 ans": 1825,
            "10 ans": 3650,
            "Total (toutes les donn√©es)": None,
        }

        if period not in periods:
            logger.warning(f"‚ö†Ô∏è P√©riode inconnue: {period}")
            return df

        days = periods[period]
        if days is None:
            return df

        # Utiliser la derni√®re date des donn√©es comme r√©f√©rence
        last_date = df["DATE"].max()
        start_date = last_date - pd.Timedelta(days=days)

        # Filtrer et trier
        filtered_df = df[df["DATE"] >= start_date].copy()
        filtered_df = filtered_df.sort_values("DATE").reset_index(drop=True)

        logger.info(f"‚úÖ Filtrage {period}: {len(filtered_df)} lignes")
        return filtered_df

    def get_available_tickers(self) -> list:
        """Retourne la liste des tickers disponibles"""
        if not self.historical_path.exists():
            return []

        tickers = []
        for file_path in self.historical_path.glob("*.parquet"):
            ticker = file_path.stem.replace("_1999_2025", "")
            tickers.append(ticker)

        return sorted(tickers)
